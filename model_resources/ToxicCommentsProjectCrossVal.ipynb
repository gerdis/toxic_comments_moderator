{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "import logging\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lade Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:\\\\Users\\\\gande\\\\Desktop\\\\ProjektKlassifikation\\\\toxic_comments_data\\\\train.csv', \n",
    "                   sep=',', header=0, quotechar= '\"', quoting=csv.QUOTE_MINIMAL, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpath = 'C:\\\\Users\\\\gande\\\\Desktop\\\\ProjektKlassifikation\\\\toxic_comments_data\\\\test.csv'\n",
    "test_df = pd.read_csv(testpath, sep=',', header=0, quotechar= '\"', quoting=csv.QUOTE_MINIMAL, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlabelpath = 'C:\\\\Users\\\\gande\\\\Desktop\\\\ProjektKlassifikation\\\\toxic_comments_data\\\\test_labels.csv'\n",
    "test_label_df = pd.read_csv(testlabelpath, sep=',', header=0, quotechar= '\"', quoting=csv.QUOTE_MINIMAL,\n",
    "                            encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entferne Kommentare mit [-1, -1, -1, -1, -1, -1] Labeling in Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_minus_ones_comments(comments_df, label_df):\n",
    "    \n",
    "    for index, row in label_df.iterrows():\n",
    "        rowlabels = [row['toxic'], row['severe_toxic'],\n",
    "                     row['obscene'], row['threat'],\n",
    "                     row['insult'], row['identity_hate']]\n",
    "        if -1 in rowlabels:\n",
    "            comments_df = comments_df.drop([index], axis=0)\n",
    "            \n",
    "    return comments_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_minus_ones_labels(label_df):\n",
    "    for index, row in label_df.iterrows():\n",
    "        rowlabels = [row['toxic'], row['severe_toxic'],\n",
    "                     row['obscene'], row['threat'],\n",
    "                     row['insult'], row['identity_hate']]\n",
    "        \n",
    "        if -1 in rowlabels:\n",
    "            label_df = label_df.drop([index], axis=0)\n",
    "            \n",
    "    return label_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1 = drop_minus_ones_comments(test_df[0:70000], test_label_df[0:70000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2 = drop_minus_ones_comments(test_df[70000:], test_label_df[70000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_df1 = drop_minus_ones_labels(test_label_df[0:70000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_df2 = drop_minus_ones_labels(test_label_df[70000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat([test_df1, test_df2])\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "test_label_df = pd.concat([test_label_df1, test_label_df2])\n",
    "test_label_df = test_label_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare train and test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "y_train = [[row[l] for l in labels] for index, row in data.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [[row[l] for l in labels] for index, row in test_label_df.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate train and test data for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([data, test_df], sort=False)\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "all_labels = y_train\n",
    "all_labels.extend(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    \n",
    "    \"\"\"remove punctuation, \n",
    "    convert to lowercase\n",
    "    \"\"\"\n",
    "    \n",
    "    corpus = []\n",
    "    for index, row in df.iterrows():\n",
    "        corpus.append([re.sub(\"[^a-zA-Z']\", ' ', \n",
    "                      row['comment_text'].lower())])\n",
    "        \n",
    "    return np.ravel(corpus).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = prepare_data(all_data)\n",
    "X = traindata\n",
    "y = all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Google vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\gande\\\\GoogleNews-vectors-negative300.bin'\n",
    "googlevecs = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define functions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_predictions(x_test, model):\n",
    "\n",
    "    predarr = np.zeros(6).reshape(1, 6)\n",
    "\n",
    "    for x in x_test:\n",
    "        comment = x.reshape(1, x.shape[0], 300)\n",
    "        prediction = np.round(model.predict(comment, steps=1))\n",
    "        predarr = np.concatenate((predarr, prediction))    \n",
    "    \n",
    "    y_pred = predarr[1:]\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_ratio(test_data, y_true, mymodel):\n",
    "    \n",
    "    comparisons = []\n",
    "    toxic = []\n",
    "\n",
    "    for idx, cl in enumerate(y_true):\n",
    "        \n",
    "        comment = test_data[idx]\n",
    "        \n",
    "        x = comment.reshape(1, comment.shape[0], 300)\n",
    "        prediction = np.round(mymodel.predict(x, steps=1))\n",
    "        prediction = prediction[0]\n",
    "            \n",
    "        same = np.array_equal(cl, prediction)\n",
    "        \n",
    "        if same and np.any(prediction):\n",
    "            toxic.append(cl)\n",
    "        \n",
    "        comparisons.append(same)\n",
    "    \n",
    "    comp = np.array(comparisons)\n",
    "    correct = np.count_nonzero(comp)\n",
    "    total = len(comp)\n",
    "    \n",
    "    print(\"correctly classified toxic comments: \", len(toxic))\n",
    "    \n",
    "    return (correct/total) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(x_test, model):\n",
    "    \n",
    "    \"\"\"compute probability scores\n",
    "    for each label\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    predarr = np.zeros(6).reshape(1, 6)\n",
    "\n",
    "    for x in x_test:\n",
    "        comment = x.reshape(1, x.shape[0], 300)\n",
    "        prediction = model.predict(comment, steps=1)\n",
    "        predarr = np.concatenate((predarr, prediction))\n",
    "    \n",
    "    y_pred = predarr[1:]\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_labels(predictions, truelabels, labelname):\n",
    "    \n",
    "    labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    labelindex = labels.index(labelname)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for idx, labelset in enumerate(predictions):\n",
    "        if labelset[labelindex] and truelabels[idx][labelindex]:\n",
    "            count+= 1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for cross validation\n",
    "\n",
    "Generate stratified folds. The following algorithm for stratified sampling of multi-label data was implemented following Sechidis et al.'s (2011) proposals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_stratification(dataset, datalabels, labelnames, k=6):\n",
    "    \n",
    "    #put all negative samples in extra list\n",
    "    all_neg = []\n",
    "    data_and_labels = [x for x in zip(dataset, datalabels)]\n",
    "    \n",
    "    for d in data_and_labels:\n",
    "        if not np.any(d[1]):\n",
    "            all_neg.append(d)\n",
    "    for f in all_neg:\n",
    "        data_and_labels.remove(f)\n",
    "    dataset = [z[0] for z in data_and_labels]\n",
    "    datalabels = [z[1] for z in data_and_labels]\n",
    "        \n",
    "    #build dictionary that will contain actual subsets\n",
    "    actual_subsets = dict()\n",
    "    for n in range(1,k+1):\n",
    "        actual_subsets[n] = []\n",
    "    \n",
    "    #Calculate desired number of samples per subset\n",
    "    subsets = dict()\n",
    "    proportion = 1/k\n",
    "    subset_size = len(dataset) * proportion\n",
    "    \n",
    "    for i in range(1,k+1):\n",
    "        \n",
    "        subsets[i] = dict()\n",
    "        subsets[i]['current_size'] = subset_size\n",
    "        \n",
    "    #Calculate desired number of samples of each label in each subset\n",
    "    current_labelcount = dict()\n",
    "    for l in labelnames:\n",
    "        #Find the examples of each label in the initial set\n",
    "        labelindex = labelnames.index(l)\n",
    "        total_count_label = len([labelset[labelindex] for labelset in datalabels if labelset[labelindex]])\n",
    "        current_labelcount[l] = total_count_label\n",
    "        for k in subsets.keys():\n",
    "            #we want the same number in all subsets if possible\n",
    "            subsets[k][l] = proportion * total_count_label\n",
    "            \n",
    "    while len(dataset) > 0:\n",
    "        #Find label with the fewest (but at least one) remaining samples, \n",
    "        nonempty = {label:count for (label, count) in current_labelcount.items() if current_labelcount[label] > 0}\n",
    "        nonzero_counts = np.array(list(nonempty.values()))\n",
    "        try:\n",
    "            sparsest = np.argmin(nonzero_counts)\n",
    "            number = np.min(nonzero_counts)\n",
    "            name_of_label = list(nonempty.keys())[sparsest]\n",
    "            index_of_label = list(current_labelcount.keys()).index(name_of_label)\n",
    "            #Then, for each sample (x, Y ) with this label, select\n",
    "            #an appropriate subset for distribution.\n",
    "            distributed_pairs = []\n",
    "            for idx, s in enumerate(dataset):\n",
    "                if datalabels[idx][index_of_label] == 1:\n",
    "                    #Find the subset with the largest number of desired samples for this label\n",
    "                    desired_numbers_label = [subsets[k][name_of_label] for k in subsets.keys()]\n",
    "                    max_desired_number = np.max(desired_numbers_label)\n",
    "                    indices_maxima = np.where(desired_numbers_label == max_desired_number)[0]\n",
    "                    howmany = len(indices_maxima)\n",
    "                    if howmany == 1:\n",
    "                        index_subset = np.argmax(desired_numbers_label)\n",
    "                        put_in_subset = list(subsets.keys())[index_subset]\n",
    "                    else:\n",
    "                        #among the tying subsets, the one with the \n",
    "                        #highest number of desired examples gets selected\n",
    "                        cand = [key for key in subsets.keys() if list(subsets.keys()).index(key) in indices_maxima]\n",
    "                        desired_numbers_total = [subsets[j]['current_size'] for j in cand]\n",
    "                        max_desired_total = np.max(desired_numbers_total)\n",
    "                        indices_maxima_total = np.where(desired_numbers_total == max_desired_total)[0]\n",
    "                        howmany_total = len(indices_maxima_total)\n",
    "                        if howmany_total == 1:\n",
    "                            index_subset_in_cand = np.argmax(desired_numbers_total)\n",
    "                            put_in_subset = cand[index_subset_in_cand]\n",
    "                        else:\n",
    "                            #pick random element of cand\n",
    "                            put_in_subset = random.choice(cand)\n",
    "                    #Once the appropriate subset is selected, we add the sample (x, Y ) \n",
    "                    #to it and remove it from D.\n",
    "                    actual_subsets[put_in_subset].append((s, datalabels[idx]))\n",
    "                    distributed_pairs.append((s, datalabels[idx]))\n",
    "                    #At the end of the iteration, we decrement the total number \n",
    "                    #of desired examples for subset m, cm\n",
    "                    subsets[put_in_subset]['current_size'] = subsets[put_in_subset]['current_size'] - 1\n",
    "                    #decrement the number of desired samples \n",
    "                    #for each label of this example in chosen subset\n",
    "                    for labelind, lab in enumerate(datalabels[idx]):\n",
    "                        if lab == 1:\n",
    "                            name = labelnames[labelind]\n",
    "                            subsets[put_in_subset][name] = subsets[put_in_subset][name] - 1\n",
    "                            current_labelcount[name] = current_labelcount[name] - 1\n",
    "                        \n",
    "            data_and_labels = [x for x in zip(dataset, datalabels)]\n",
    "            for p in distributed_pairs:\n",
    "                data_and_labels.remove(p)\n",
    "            dataset = [z[0] for z in data_and_labels]\n",
    "            datalabels = [z[1] for z in data_and_labels]\n",
    "        \n",
    "        except ValueError:\n",
    "            break\n",
    "            \n",
    "    #Samples that are not annotated with any label are distributed so as to \n",
    "    #balance the desired number of examples at each subset. \n",
    "    negs_per_subset = math.floor(len(all_neg) * proportion)\n",
    "    if negs_per_subset:\n",
    "        for everykey in actual_subsets.keys():\n",
    "            add_negatives = random.sample(all_neg,  negs_per_subset)\n",
    "            actual_subsets[everykey].extend(add_negatives)\n",
    "        \n",
    "    #SHUFFLE ACTUAL SUBSETS\n",
    "    for v in actual_subsets.values():\n",
    "        random.shuffle(v)\n",
    "    \n",
    "    return actual_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified = iterative_stratification(X, y, labelnames=labels, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'toxic': 4277,\n",
       "  'severe_toxic': 392,\n",
       "  'obscene': 2428,\n",
       "  'threat': 137,\n",
       "  'insult': 2261,\n",
       "  'identity_hate': 424,\n",
       "  'total': 44713},\n",
       " 2: {'toxic': 4277,\n",
       "  'severe_toxic': 392,\n",
       "  'obscene': 2428,\n",
       "  'threat': 138,\n",
       "  'insult': 2261,\n",
       "  'identity_hate': 423,\n",
       "  'total': 44699},\n",
       " 3: {'toxic': 4277,\n",
       "  'severe_toxic': 393,\n",
       "  'obscene': 2428,\n",
       "  'threat': 138,\n",
       "  'insult': 2261,\n",
       "  'identity_hate': 423,\n",
       "  'total': 44701},\n",
       " 4: {'toxic': 4276,\n",
       "  'severe_toxic': 392,\n",
       "  'obscene': 2428,\n",
       "  'threat': 138,\n",
       "  'insult': 2261,\n",
       "  'identity_hate': 423,\n",
       "  'total': 44727},\n",
       " 5: {'toxic': 4277,\n",
       "  'severe_toxic': 393,\n",
       "  'obscene': 2428,\n",
       "  'threat': 138,\n",
       "  'insult': 2260,\n",
       "  'identity_hate': 424,\n",
       "  'total': 44708}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#control distribution\n",
    "distribution = dict()\n",
    "for key in stratified.keys():\n",
    "    distribution[key] = dict()\n",
    "    for l in labels:\n",
    "        labelindex = labels.index(l)\n",
    "        number = np.count_nonzero([x[1][labelindex] for x in stratified[key]])\n",
    "        distribution[key][l] = number\n",
    "    distribution[key]['total'] = len(stratified[key])\n",
    "distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert words in comments to vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_w2v_rep(stratdict, model, lemmatize=True, pretrained=googlevecs):    \n",
    "    \n",
    "    newstratdict = dict()\n",
    "    \n",
    "    for key in stratdict.keys():\n",
    "        newstratdict[key] = []\n",
    "        \n",
    "        for pair in stratdict[key]:\n",
    "            splitcomment = pair[0].split()\n",
    "            commentlist = []\n",
    "            \n",
    "            if lemmatize:\n",
    "                verbs_lemmatized = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in splitcomment]\n",
    "                splitcomment = [wordnet_lemmatizer.lemmatize(word, pos='n') for word in verbs_lemmatized]\n",
    "            \n",
    "            for word in splitcomment:\n",
    "                try:\n",
    "                    commentlist.append(model[word])\n",
    "\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        commentlist.append(pretrained[word])\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "            \n",
    "            if len(commentlist) != 0:\n",
    "                commentarr = np.array(commentlist)\n",
    "                newstratdict[key].append((commentarr, pair[1]))\n",
    "            \n",
    "    return newstratdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_folds = convert_to_w2v_rep(stratified, googlevecs, lemmatize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, Dense, GlobalMaxPooling1D, Dropout\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(stratified, epochs=5):\n",
    "\n",
    "    accuracy = []\n",
    "    precision_macro_weighted = []\n",
    "    precision_micro = []\n",
    "    recall_macro_weighted = []\n",
    "    recall_micro = []\n",
    "    f1_macro_weighted = []\n",
    "    f1_micro = []\n",
    "    my_hamming_loss = []\n",
    "    \n",
    "    correct_toxic = []\n",
    "    correct_severe_toxic = []\n",
    "    correct_obscene = []\n",
    "    correct_threat = []\n",
    "    correct_insult = []\n",
    "    correct_hate = []\n",
    "    \n",
    "\n",
    "    for fold in stratified.keys():\n",
    "        trainfolds = [f for f in stratified.keys() if f != fold]\n",
    "        train_data = []\n",
    "        train_labels = []\n",
    "        for t in trainfolds:\n",
    "            data_in_t = [pair[0] for pair in stratified[t]]\n",
    "            labels_in_t = [pair[1] for pair in stratified[t]]\n",
    "            train_data.extend(data_in_t)\n",
    "            train_labels.extend(labels_in_t)\n",
    "        train_data = np.array(train_data)\n",
    "        train_labels = np.array(train_labels)\n",
    "        train_steps = len(train_labels)\n",
    "        \n",
    "        val_data = np.array([p[0] for p in stratified[fold]])\n",
    "        val_labels = np.array([p[1] for p in stratified[fold]])\n",
    "        \n",
    "        #build model\n",
    "        learning_rate = 1e-3\n",
    "        my_sgd = optimizers.SGD(lr=learning_rate, decay=0.0, momentum=0.9)\n",
    "\n",
    "        i = Input((None, 300))\n",
    "\n",
    "        conv1 = Conv1D(200, kernel_size=1, padding='valid', activation='relu')(i)\n",
    "        conv2 = Conv1D(200, kernel_size=2, padding='same', activation='relu')(conv1)\n",
    "        pool = GlobalMaxPooling1D()(conv2)\n",
    "        drop = Dropout(0.25)(pool)\n",
    "\n",
    "        d = Dense(400, activation='relu')(drop)\n",
    "        drop2 = Dropout(0.25)(d)\n",
    "\n",
    "        o = Dense(6, activation='sigmoid')(drop2)      \n",
    "\n",
    "        My_CNN = Model(i, o)\n",
    "\n",
    "        My_CNN.compile(loss='binary_crossentropy', optimizer=my_sgd, metrics=['accuracy'])\n",
    "\n",
    "        def generate_inputs():\n",
    "\n",
    "            while True:\n",
    "\n",
    "                for pair in zip(train_data, train_labels):\n",
    "                    x_train = pair[0].reshape(1, pair[0].shape[0], 300)\n",
    "                    y_train = pair[1].reshape(1, 6)\n",
    "                    yield x_train, y_train\n",
    "                    \n",
    "        #train_model\n",
    "        My_CNN.fit_generator(generate_inputs(), steps_per_epoch=train_steps, verbose=0, epochs=epochs)\n",
    "\n",
    "\n",
    "        #evaluate model\n",
    "        y_pred = all_predictions(val_data, My_CNN)\n",
    "        acc = exact_match_ratio(val_data, val_labels, My_CNN)\n",
    "        accuracy.append(acc)\n",
    "        prec_weighted = precision_score(val_labels, y_pred, average='weighted')\n",
    "        precision_macro_weighted.append(prec_weighted)\n",
    "        prec_micro = precision_score(val_labels, y_pred, average='micro')\n",
    "        precision_micro.append(prec_micro)\n",
    "        rec_weighted = recall_score(val_labels, y_pred, average='weighted')\n",
    "        recall_macro_weighted.append(rec_weighted)\n",
    "        rec_micro = recall_score(val_labels, y_pred, average='micro')\n",
    "        recall_micro.append(rec_micro)\n",
    "        f1_weighted = f1_score(val_labels, y_pred, average='weighted')\n",
    "        f1_macro_weighted.append(f1_weighted)\n",
    "        f1_mic = f1_score(val_labels, y_pred, average='micro')\n",
    "        f1_micro.append(f1_mic)\n",
    "        hamm_loss = hamming_loss(val_labels, y_pred)\n",
    "        my_hamming_loss.append(hamm_loss)\n",
    "        \n",
    "        #calculate number of correctly predicted comments for each label\n",
    "        corr_toxic = compare_labels(y_pred, val_labels, 'toxic')\n",
    "        correct_toxic.append(corr_toxic)\n",
    "        corr_sev_toxic = compare_labels(y_pred, val_labels, 'severe_toxic')\n",
    "        correct_severe_toxic.append(corr_sev_toxic)\n",
    "        corr_obscene = compare_labels(y_pred, val_labels, 'obscene')\n",
    "        correct_obscene.append(corr_obscene)\n",
    "        corr_threat = compare_labels(y_pred, val_labels, 'threat')\n",
    "        correct_threat.append(corr_threat)\n",
    "        corr_insult = compare_labels(y_pred, val_labels, 'insult')\n",
    "        correct_insult.append(corr_insult)\n",
    "        corr_hate = compare_labels(y_pred, val_labels, 'identity_hate')\n",
    "        correct_hate.append(corr_hate)\n",
    "        \n",
    "        print(\"Finished training fold\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"mean accuracy: %.2f%% (+/- %.2f)\" % (np.mean(accuracy), np.std(accuracy)))\n",
    "    print(\"mean weighted macro-averaged precision: %.3f (+/- %.3f)\" % (np.mean(precision_macro_weighted), np.std(precision_macro_weighted)))\n",
    "    print(\"mean micro-averaged precision: %.3f (+/- %.3f)\" % (np.mean(precision_micro), np.std(precision_micro)))\n",
    "    print(\"mean weighted macro-averaged recall: %.3f (+/- %.3f)\" % (np.mean(recall_macro_weighted), np.std(recall_macro_weighted)))\n",
    "    print(\"mean micro-averaged recall: %.3f (+/- %.3f)\" % (np.mean(recall_micro), np.std(recall_micro)))\n",
    "    print(\"mean weighted macro-averaged F1 score: %.3f (+/- %.3f)\" % (np.mean(f1_macro_weighted), np.std(f1_macro_weighted)))\n",
    "    print(\"mean micro-averaged F1 score: %.3f (+/- %.3f)\" % (np.mean(f1_micro), np.std(f1_micro)))\n",
    "    print(\"mean hamming loss: %.3f (+/- %.3f)\" % (np.mean(my_hamming_loss), np.std(my_hamming_loss)))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Average number of correctly predicted samples per label: \")\n",
    "    print(\"\\n\")\n",
    "    print(\"toxic: \", int(np.mean(correct_toxic)))\n",
    "    print(\"severe_toxic: \", int(np.mean(correct_severe_toxic)))\n",
    "    print(\"obscene: \", int(np.mean(correct_obscene)))\n",
    "    print(\"threat: \", int(np.mean(correct_threat)))\n",
    "    print(\"insult: \", int(np.mean(correct_insult)))\n",
    "    print(\"identity_hate: \", int(np.mean(correct_hate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correctly classified toxic comments:  1515\n",
      "Finished training fold\n",
      "correctly classified toxic comments:  1459\n",
      "Finished training fold\n",
      "correctly classified toxic comments:  1411\n",
      "Finished training fold\n",
      "correctly classified toxic comments:  1389\n",
      "Finished training fold\n",
      "correctly classified toxic comments:  1492\n",
      "Finished training fold\n",
      "\n",
      "\n",
      "mean accuracy: 91.89% (+/- 0.08)\n",
      "mean weighted macro-averaged precision: 0.795 (+/- 0.012)\n",
      "mean micro-averaged precision: 0.801 (+/- 0.012)\n",
      "mean weighted macro-averaged recall: 0.661 (+/- 0.014)\n",
      "mean micro-averaged recall: 0.661 (+/- 0.014)\n",
      "mean weighted macro-averaged F1 score: 0.713 (+/- 0.005)\n",
      "mean micro-averaged F1 score: 0.724 (+/- 0.005)\n",
      "mean hamming loss: 0.019 (+/- 0.000)\n",
      "\n",
      "\n",
      "Average number of correctly predicted samples per label: \n",
      "\n",
      "\n",
      "toxic:  3029\n",
      "severe_toxic:  60\n",
      "obscene:  1781\n",
      "threat:  12\n",
      "insult:  1513\n",
      "identity_hate:  161\n"
     ]
    }
   ],
   "source": [
    "cross_validate(all_folds, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare results to dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_dummy = iterative_stratification(X, y, labelnames=labels, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_folds_dummy = convert_to_w2v_rep(stratified_dummy, googlevecs, lemmatize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_dummy(stratified, pred_strategy):\n",
    "\n",
    "    accuracy = []\n",
    "    precision_macro_weighted = []\n",
    "    precision_micro = []\n",
    "    recall_macro_weighted = []\n",
    "    recall_micro = []\n",
    "    f1_macro_weighted = []\n",
    "    f1_micro = []\n",
    "    my_hamming_loss = []\n",
    "    \n",
    "    correct_toxic = []\n",
    "    correct_severe_toxic = []\n",
    "    correct_obscene = []\n",
    "    correct_threat = []\n",
    "    correct_insult = []\n",
    "    correct_hate = []\n",
    "    \n",
    "\n",
    "    for fold in stratified.keys():\n",
    "        trainfolds = [f for f in stratified.keys() if f != fold]\n",
    "        train_data = []\n",
    "        train_labels = []\n",
    "        for t in trainfolds:\n",
    "            data_in_t = [pair[0] for pair in stratified[t]]\n",
    "            labels_in_t = [pair[1] for pair in stratified[t]]\n",
    "            train_data.extend(data_in_t)\n",
    "            train_labels.extend(labels_in_t)\n",
    "        train_data = np.array(train_data)\n",
    "        train_labels = np.array(train_labels)\n",
    "        #train_steps = len(train_labels)\n",
    "        \n",
    "        val_data = np.array([p[0] for p in stratified[fold]])\n",
    "        val_labels = np.array([p[1] for p in stratified[fold]])\n",
    "        \n",
    "        #initiate classifier\n",
    "        clf = DummyClassifier(strategy=pred_strategy)\n",
    "                    \n",
    "        #fit classifier\n",
    "        clf.fit(train_data, train_labels)\n",
    "\n",
    "        #evaluate model\n",
    "        y_pred = clf.predict(val_data)\n",
    "        acc = clf.score(val_data, val_labels)\n",
    "        accuracy.append(acc)\n",
    "        prec_weighted = precision_score(val_labels, y_pred, average='weighted')\n",
    "        precision_macro_weighted.append(prec_weighted)\n",
    "        prec_micro = precision_score(val_labels, y_pred, average='micro')\n",
    "        precision_micro.append(prec_micro)\n",
    "        rec_weighted = recall_score(val_labels, y_pred, average='weighted')\n",
    "        recall_macro_weighted.append(rec_weighted)\n",
    "        rec_micro = recall_score(val_labels, y_pred, average='micro')\n",
    "        recall_micro.append(rec_micro)\n",
    "        f1_weighted = f1_score(val_labels, y_pred, average='weighted')\n",
    "        f1_macro_weighted.append(f1_weighted)\n",
    "        f1_mic = f1_score(val_labels, y_pred, average='micro')\n",
    "        f1_micro.append(f1_mic)\n",
    "        hamm_loss = hamming_loss(val_labels, y_pred)\n",
    "        my_hamming_loss.append(hamm_loss)\n",
    "        \n",
    "        #calculate number of correctly predicted comments for each label\n",
    "        corr_toxic = compare_labels(y_pred, val_labels, 'toxic')\n",
    "        correct_toxic.append(corr_toxic)\n",
    "        corr_sev_toxic = compare_labels(y_pred, val_labels, 'severe_toxic')\n",
    "        correct_severe_toxic.append(corr_sev_toxic)\n",
    "        corr_obscene = compare_labels(y_pred, val_labels, 'obscene')\n",
    "        correct_obscene.append(corr_obscene)\n",
    "        corr_threat = compare_labels(y_pred, val_labels, 'threat')\n",
    "        correct_threat.append(corr_threat)\n",
    "        corr_insult = compare_labels(y_pred, val_labels, 'insult')\n",
    "        correct_insult.append(corr_insult)\n",
    "        corr_hate = compare_labels(y_pred, val_labels, 'identity_hate')\n",
    "        correct_hate.append(corr_hate)\n",
    "        \n",
    "               \n",
    "    print(\"\\n\")\n",
    "    print(\"mean accuracy: %.2f%% (+/- %.2f)\" % (np.mean(accuracy), np.std(accuracy)))\n",
    "    print(\"mean weighted macro-averaged precision: %.3f (+/- %.3f)\" % (np.mean(precision_macro_weighted), np.std(precision_macro_weighted)))\n",
    "    print(\"mean micro-averaged precision: %.3f (+/- %.3f)\" % (np.mean(precision_micro), np.std(precision_micro)))\n",
    "    print(\"mean weighted macro-averaged recall: %.3f (+/- %.3f)\" % (np.mean(recall_macro_weighted), np.std(recall_macro_weighted)))\n",
    "    print(\"mean micro-averaged recall: %.3f (+/- %.3f)\" % (np.mean(recall_micro), np.std(recall_micro)))\n",
    "    print(\"mean weighted macro-averaged F1 score: %.3f (+/- %.3f)\" % (np.mean(f1_macro_weighted), np.std(f1_macro_weighted)))\n",
    "    print(\"mean micro-averaged F1 score: %.3f (+/- %.3f)\" % (np.mean(f1_micro), np.std(f1_micro)))\n",
    "    print(\"mean hamming loss: %.3f (+/- %.3f)\" % (np.mean(my_hamming_loss), np.std(my_hamming_loss)))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Average number of correctly predicted samples per label: \")\n",
    "    print(\"\\n\")\n",
    "    print(\"toxic: \", int(np.mean(correct_toxic)))\n",
    "    print(\"severe_toxic: \", int(np.mean(correct_severe_toxic)))\n",
    "    print(\"obscene: \", int(np.mean(correct_obscene)))\n",
    "    print(\"threat: \", int(np.mean(correct_threat)))\n",
    "    print(\"insult: \", int(np.mean(correct_insult)))\n",
    "    print(\"identity_hate: \", int(np.mean(correct_hate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "mean accuracy: 0.72% (+/- 0.00)\n",
      "mean weighted macro-averaged precision: 0.068 (+/- 0.002)\n",
      "mean micro-averaged precision: 0.068 (+/- 0.002)\n",
      "mean weighted macro-averaged recall: 0.068 (+/- 0.002)\n",
      "mean micro-averaged recall: 0.068 (+/- 0.002)\n",
      "mean weighted macro-averaged F1 score: 0.068 (+/- 0.002)\n",
      "mean micro-averaged F1 score: 0.068 (+/- 0.002)\n",
      "mean hamming loss: 0.069 (+/- 0.000)\n",
      "\n",
      "\n",
      "Average number of correctly predicted samples per label: \n",
      "\n",
      "\n",
      "toxic:  416\n",
      "severe_toxic:  5\n",
      "obscene:  136\n",
      "threat:  0\n",
      "insult:  114\n",
      "identity_hate:  4\n"
     ]
    }
   ],
   "source": [
    "cross_validate_dummy(all_folds_dummy, 'stratified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "mean accuracy: 0.02% (+/- 0.00)\n",
      "mean weighted macro-averaged precision: 0.067 (+/- 0.000)\n",
      "mean micro-averaged precision: 0.037 (+/- 0.000)\n",
      "mean weighted macro-averaged recall: 0.501 (+/- 0.005)\n",
      "mean micro-averaged recall: 0.501 (+/- 0.005)\n",
      "mean weighted macro-averaged F1 score: 0.116 (+/- 0.001)\n",
      "mean micro-averaged F1 score: 0.069 (+/- 0.001)\n",
      "mean hamming loss: 0.500 (+/- 0.000)\n",
      "\n",
      "\n",
      "Average number of correctly predicted samples per label: \n",
      "\n",
      "\n",
      "toxic:  2127\n",
      "severe_toxic:  200\n",
      "obscene:  1222\n",
      "threat:  69\n",
      "insult:  1133\n",
      "identity_hate:  212\n"
     ]
    }
   ],
   "source": [
    "cross_validate_dummy(all_folds_dummy, 'uniform')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
